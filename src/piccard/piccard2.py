import numpy as np
import math
from tscluster.opttscluster import OptTSCluster
from tscluster.preprocessing.utils import load_data, tnf_to_ntf, ntf_to_tnf

import warnings
warnings.filterwarnings('ignore')

# unused
import pandas as pd
import geopandas as gpd
import networkx as nx
import re
import matplotlib.pyplot as plt

def clustering_prep(network_table, id, cols=[]):
    '''
    Converts a piccard network table into a 3d numpy array of all possible paths and their corresponding
    features. This will be used for clustering with tscluster.
    The user can (optionally) input a list of columns that they want to be considered in the clustering algorithm, 
    and the function will check that these columns are valid.

    Note that you must run pc.create_network_table() before this function.

    Inputs:
    - network_table: The result of pc.create_network_table().
    - id: The same id inputted into pc.create_network_table().
    - cols (optional): A list of the names of network table columns that should be considered in
    the clustering algorithm. If none, every numerical feature will be considered. This is
    not recommended as many numerical features, such as network level, have little bearing on the data.

    Returns a tuple of a 3d numpy array and a corresponding dictionary of labels showing
    the shape of the array.
    '''
    # default to considering all features
    if cols == []:
        cols = network_table.columns.to_list()

    # Find all years present in the data. These will be used as timesteps for tscluster.
    year_cols = [col for col in network_table.columns.to_list() if id in col]
    years = sorted(list({col[-4:] for col in year_cols}))

    # Filter columns
    filtered_cols = filter_columns(network_table, years, cols)

    # Extract features for each year and add them to a 2D array representing that year. 
    # Then add that array to a list of arrays representing the 3D array used for tscluster.
    list_of_arrays = []
    for year in years:
        year_statistics = network_table[[col for col in filtered_cols[0] if year in col]].to_numpy()
        list_of_arrays.append(year_statistics)
    
    # Filter out entities whose features are entirely NaN
    # Run load_data now so we get access to variables necessary for tnf_to_ntf
    list_of_arrays = load_data(list_of_arrays)[0] 
    ntf_list_of_arrays = tnf_to_ntf(list_of_arrays)
    count = -1
    for entity in ntf_list_of_arrays:
        count += 1
        number_in_entity = False
        for i in entity.flat:
            if not np.isnan(i):
                number_in_entity = True
                break
        if not number_in_entity:
            np.delete(ntf_list_of_arrays, count, 0)
    list_of_arrays = ntf_to_tnf(ntf_list_of_arrays)
    
    # Return the final numpy array and create a corresponding label dictionary.
    # This can then be preprocessed using tscluster's scalers.
    label_dict = {'T': years, 'N': [f'Path {i}' for i in range(count + 1)], 'F': filtered_cols[1]}
    return (list_of_arrays, label_dict)


def cluster(network_table, G, id, num_clusters, scheme='z1c1', arr=None, label_dict=None):
    '''
    Runs one of tscluster's clustering algorithms (default is fully dynamic clustering or 'z1c1')
    and adds the resulting cluster assignments to the network table and nodes as an additional feature.
    Information about the different clustering algorithms is available here: https://tscluster.readthedocs.io/en/latest/introduction.html
    We recommend either Sequential Label Analysis ('z1c0') or the default 'z1c1'.

    Users can choose to only input the network table, in which case clustering_prep will be run for them with the default columns,
    or they can choose to run clustering_prep on their own and then have the option to apply one or both of the
    normalization methods available in tscluster.preprocessing.utils.

    Inputs:
    - network_table: The result of pc.create_network_table().
    - G: The result of pc.create_network().
    - id: The same id inputted into pc.create_network_table().
    - num_clusters: The number of clusters that the algorithm will find.
    - scheme (optional): the clustering scheme. See the first paragraph for more information. Default is 'z1c1'.
    - arr (optional): the array of data to be clustered. If none, arr and label_dict will be generated by running
    pc.clustering_prep() with the default columns. See the pc.clustering_prep() documentation for why we DO NOT
    recommend leaving this blank.
    - label_dict (optional): the label dictionary corresponding to the data array.

    Returns an OptTSCluster object with useful labels, cluster assignments, etc for future visualizations.

    Note that you may need to download a Gurobi academic licence to run the clustering algorithm. More information here:
    https://www.gurobi.com/academia/academic-program-and-licenses/
    '''
    # Get the data into the correct format. See the documentation for clustering_prep
    if arr is None and label_dict is None:
        arr, label_dict = clustering_prep(network_table, id)

    # Set every nan value in the array to an absurdly impossible value so it doesn't throw off clustering
    arr = np.nan_to_num(arr, nan = -1000000)
    
    # Initialize the model
    opt_ts = OptTSCluster(
        n_clusters=num_clusters,
        scheme=scheme,
        n_allow_assignment_change=None # Allow as many changes as possible
    )
    # Assign clusters
    opt_ts.fit(arr, label_dict=label_dict)

    # Add cluster assignments to network table
    cluster_assignments_table = opt_ts.get_named_labels(label_dict=label_dict)
    years = label_dict['T']
    for year in years:
        network_table[f'cluster_assignment_{year}'] = list(cluster_assignments_table[year])

    # Add cluster assignments to graph nodes
    nodes_list = list(G.nodes(data=True))
    for node in nodes_list:
            year = node[0][:4]
            cluster = network_table.loc[network_table[f'geouid_{year}'] == node[0]]
            if len(cluster) != 0:
                cluster = int(cluster.iloc[0][f'cluster_assignment_{year}'])
                dict = opt_ts.get_named_cluster_centers(label_dict=label_dict)[cluster].loc[year]
                # figure out which cluster to assign a node to if it's already been assigned to a different cluster
                if 'cluster_assignment' in node[1] and node[1]['cluster_assignment'] != cluster:
                    old_dict = opt_ts.get_named_cluster_centers(label_dict=label_dict)[node[1]['cluster_assignment']].loc[year]
                    # comparing distances between clusters
                    old_cluster_distance = 0
                    new_cluster_distance = 0
                    for i in range(len(dict)):
                        old_cluster_distance += (math.abs(int(node[1][label_dict['F'][i]]) - int(old_dict[i])))
                        new_cluster_distance += (math.abs(int(node[1][label_dict['F'][i]]) - int(dict[i])))
                    if old_cluster_distance < new_cluster_distance:
                        cluster = node[1]['cluster_assignment']
                node[1]['cluster_assignment'] = cluster
            elif 'cluster_assignment' not in node[1]:
                node[1]['cluster_assignment'] = np.nan
    
    return opt_ts


# Helpers

def filter_columns(network_table, years, cols=[]):
    '''
    Checks that the list of columns with data to be clustered is valid in the following ways:
    - Makes sure all the data in the columns are numerical or nan
    - Makes sure there is a version of each column for every year
    Returns a tuple of the final filtered list of columns and the column labels that will
    be used for the label dictionary.
    '''
    # Only add features that are numerical or nan. the user should have selected accordingly
    # but this is a sanity check
    col_list = []

    for col in cols:
        if col in network_table.columns.to_list():
            non_numerical_val_in_col = False
            for entry in network_table[col]:
                if isinstance(entry, str) and '_' in entry: # make sure underscores don't get converted to numbers
                    non_numerical_val_in_col = True
                    break
                try:
                    int(entry)  # see if it is either an int or an int masquerading as a string
                except ValueError:
                    try:
                        float(entry)  # see if it is either a float or a float masquerading as a string
                    except ValueError:
                        if entry != 'NaN' and entry != 'nan': # see if it is nan
                            non_numerical_val_in_col = True
                            break
            if not non_numerical_val_in_col:
                col_list.append(col)

    # Only add features for which there are variables in every year. Otherwise the shape of
    # the 3D array used for tscluster will not make sense.
    # note: we can improve on this with some version of the ppandas library (https://link.springer.com/article/10.1007/s10618-024-01054-7)
    cols_in_every_year = []
    features_list = [] # for the label dictionary
    add_to_list = True
    col_names_without_year = list(dict.fromkeys([col[:-4] for col in col_list])) # remove duplicates while preserving original order
    for col in col_names_without_year:
        add_to_list = True
        for year in years:
            if f"{col}{year}" not in col_list:
                add_to_list = False
                break
        for year in years:
            if add_to_list:
                if col[:-1] not in features_list:
                    features_list.append(col[:-1])
                cols_in_every_year.append(f"{col}{year}")

    return (cols_in_every_year, features_list)
